# LinearRegression
Linear Regression using Gradient Descent (Part 1)
Linear Regression using ML Library (Part 2)

Environment Setup

Python version: 
Use Python 3.9 or newer.

Required packages: 
numpy, pandas, scikit-learn, seaborn, matplotlib

Install with: 
pip install numpy pandas scikit-learn seaborn matplotlib

Dataset: Auto MPG (UCI Machine Learning Repository).
The scripts load the dataset from a public URL (e.g., a GitHub raw link). You do not need to download the dataset manually.


Part 1 – Custom Gradient Descent Linear Regression

Main file:

part1.ipynb

What part1.ipynb does:

Loads and preprocesses the Auto MPG data (handles missing values, encodes categorical features, scales numeric features).

Splits the data into training and test sets (80% train, 20% test) with a fixed random seed.

Defines and uses a custom LinearRegressionGD class that implements linear regression using gradient descent from scratch.

Runs a grid of hyperparameters (learning rate and number of iterations).

Computes train and test mean squared error (MSE) for each hyperparameter combination.

Prints the results and saves them to a CSV log file named:

part1_hyperparam_log.csv

with columns:
learning_rate, iterations, train_mse, test_mse

Trains a final model using the best hyperparameters and reports final train and test MSE.

Generates plots (for example: correlation heatmap, MPG distribution, and optionally MSE vs iterations, predicted vs actual, residual plots) and saves them as image files.

How to run Part 1:

python part1.ipynb
Part 2 – Linear Regression Using ML Library

Main file:

part2.ipynb

Libraries used (in addition to numpy, pandas, matplotlib, seaborn, csv):

From scikit-learn:

sklearn.linear_model.SGDRegressor

sklearn.preprocessing.StandardScaler

sklearn.model_selection.train_test_split

sklearn.metrics (mean_squared_error, r2_score, explained_variance_score)

What part2.ipynb does:

Loads and preprocesses the Auto MPG data in the same way as Part 1 (to ensure a fair comparison).

Splits the data into training and test sets using the same random state.

Scales the features using StandardScaler.

Uses SGDRegressor to perform linear regression with stochastic gradient descent.

Runs a grid of hyperparameters (for example: alpha, max_iter, eta0 with learning_rate="constant").

Computes train and test MSE for each hyperparameter combination.

Prints the results and saves them to a CSV log file named:

part2_sgd_hyperparam_log.csv

with columns:
alpha, max_iter, eta0, train_mse, test_mse

Chooses the best hyperparameters based on test MSE.

Trains a final SGDRegressor model with the best hyperparameters.

Reports:

Train and test MSE

Train and test R^2

Train and test explained variance

Model coefficients (weights) and intercept

Generates plots such as:

Actual vs predicted MPG (test set)

Residuals vs predicted MPG

MPG vs key features (for example, weight, horsepower) with regression lines
and saves them as image files.

How to run Part 2:

python part2.ipynb
Outputs and How to Use Them

Log files:

part1_hyperparam_log.csv: hyperparameters and errors for the custom gradient descent model.

part2_sgd_hyperparam_log.csv: hyperparameters and errors for the SGDRegressor model.

Plots:

Image files generated by part1.py and part2.py (for example: heatmaps, histograms, predicted vs actual plots, residual plots).

Use these logs and plots in your report to:

Show how you tuned hyperparameters.

Compare performance between your custom gradient descent model (Part 1) and the library model (Part 2) using MSE, R^2, explained variance, and coefficients.

Answer the assignment questions about whether you are satisfied that you have found the best solution in each part.

